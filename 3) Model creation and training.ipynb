{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017db8f8",
   "metadata": {},
   "source": [
    "# 3 Model creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f7a785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "\n",
    "from utils import dataset_treatment, dataset_treatment_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b85e3cf",
   "metadata": {},
   "source": [
    "Now that we found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c801a036",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca1ed528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (18,19) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "azdias = pd.read_csv('data/Udacity_AZDIAS_052018.csv', sep=';') # We load azdias only to facilitate the cleaning\n",
    "mailout_data = pd.read_csv('data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d18c8e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    42430\n",
      "1      532\n",
      "Name: RESPONSE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(mailout_data.RESPONSE.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57fba6",
   "metadata": {},
   "source": [
    "As we can see the data is highly imbalanced. 98.7% of the training examples are labelled as \"0\" or \"not responsive\". This will need to be accounted for.  \n",
    "First, let's clean our dataset and split the labels from the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f558226",
   "metadata": {},
   "source": [
    "### Splitting and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486677bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting columns...\n",
      "Deleting rows...\n",
      "Encoding...\n"
     ]
    }
   ],
   "source": [
    "azidias1, mailout_data = dataset_treatment(azdias, mailout_data, cust=False)\n",
    "azidias1 = None # Clear space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63979dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.DataFrame(mailout_data[\"RESPONSE\"])\n",
    "del mailout_data[\"RESPONSE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7864bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33837, 354)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mailout_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "959fe5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33837, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33f2f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split our dataset into 80% training and 20% validation.\n",
    "X_train, X_val, Y_train, Y_val = sklearn.model_selection.train_test_split(mailout_data, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c108e00",
   "metadata": {},
   "source": [
    "### Save the data locally \n",
    "Now, let's save our train and validation data to csv files. Note that we make sure not to include header information or an index as this is required by the built in algorithms provided by Amazon. Also, for the train and validation data, it is assumed that the first entry in each row is the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6de3e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data_model\"\n",
    "\n",
    "pd.concat([Y_val, X_val], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([Y_train, X_train], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ccf4fa",
   "metadata": {},
   "source": [
    "### Upload to S3 \n",
    "Since we are currently running inside of a SageMaker session, we can use the object which represents this session to upload our data to the 'default' S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55fc530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the session and role\n",
    "session = sagemaker.Session() \n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6562fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'capstone-Arvato'\n",
    "\n",
    "val_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e83dfc",
   "metadata": {},
   "source": [
    "### Models design\n",
    "Two different types of model will be investigated. We will use SageMaker's hyperparameter tuning functionality to investigate the possible XGBoost architectures and retain the one that performs the best on the validation set.\n",
    "\n",
    "1) An XGBoost model  \n",
    "XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework.\n",
    "When it comes to small-to-medium structured/tabular data, decision tree based algorithms are considered best-in-class right now.  \n",
    "The investigated hyperparameters will be: \n",
    "\n",
    "- The \"max_depth\" of the tree.\n",
    "- The \"eta\" (learning rate equivalent).\n",
    "- The \"min_child_weight\".\n",
    "- The \"subsample\" ratio of training instances (to prevent overfitting).\n",
    "- \"gamma\", the minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "\n",
    "2) A custom ANN model   \n",
    "A simple, classic neural net composed of 4 fully connected layers.\n",
    "This model won't be hyperparametrized.\n",
    "  \n",
    "### Metric\n",
    "As stated earlier, the data is highly imbalanced. We have more thn 40 thousand - 0 responses and only 532 - 1 responses. Accuracy will be a bad metric to choose for this problem since the accuracy might always be more than 98% even if the model predicts all zeros.  \n",
    "\n",
    "To address this imbalance while evaluating the model we need to chose a metric which will take this class imbalance into accounts. The usual metrics used for imbalanced classification are Precision and Recall or Area under Receiver Operating Curve (AUROC). The AUROC metric considers both true positive rate and false positive rate. It is a good choice for this problem, since we want to be able to correctly predict both cases i.e. whether a person becomes a customer or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afacc706",
   "metadata": {},
   "source": [
    "### 0) Benchmark\n",
    "First, let's create a simple Logistic regression model on unscaled data to set a benchmark performance that we will aim to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c7ef5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c2502f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df22b814",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict_proba(X_val)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "418e024f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline AUROC -  0.6848631867634674\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline AUROC - \", roc_auc_score(Y_val, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a06b6",
   "metadata": {},
   "source": [
    "Our baseline is therefore an AUROC of 68%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a08de",
   "metadata": {},
   "source": [
    "### 1) XGBoost model \n",
    "\n",
    "Now, let's explore if we can do better using and XGBoost model.  \n",
    "The training/hyperparameter-tuning will be performed using the high level SageMaker API.  \n",
    "To construct an estimator, the object which we wish to train, we need to provide the location of a container which contains the training code. Since XGBoost is a built in algorithm this container is provided by Amazon. To do so, we will be using the get_image_uri method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c00b9b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As stated above, we use this utility method to construct the image name for the training container.\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", session.boto_region_name, \"latest\")\n",
    "\n",
    "# Now that we know which container to use, we can construct the estimator object.\n",
    "xgb = sagemaker.estimator.Estimator(container, # The name of the training container\n",
    "                                    role,      # The IAM role to use (our current role in this case)\n",
    "                                    instance_count=1, # The number of instances to use for training\n",
    "                                    instance_type='ml.m4.xlarge', # The type of instance ot use for training\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix), # Where to save the output (the model artifacts)\n",
    "                                    sagemaker_session=session) # The current SageMaker session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b6238",
   "metadata": {},
   "source": [
    "Before beginning the hyperparameter tuning, we should make sure to set any model specific hyperparameters that we wish to have default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d19b7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(max_depth=7,\n",
    "                        eta=0.39,\n",
    "                        gamma=0.98,\n",
    "                        min_child_weight=9,\n",
    "                        subsample=0.892,\n",
    "                        objective='binary:logistic', # 0/1 predictions\n",
    "                        eval_metric='auc',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27707733",
   "metadata": {},
   "source": [
    "Now that we have our estimator object completely set up, it is time to create the hyperparameter tuner. To do this we need to construct a new object which contains each of the parameters we want SageMaker to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7009f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyperparameter_tuner = HyperparameterTuner(estimator = xgb, # The estimator object to use as the basis for the training jobs.\n",
    "                                               objective_metric_name = 'validation:auc', # The metric used to compare trained models.\n",
    "                                               objective_type = 'Maximize', # Whether we wish to minimize or maximize the metric.\n",
    "                                               max_jobs = 10, # The total number of models to train\n",
    "                                               max_parallel_jobs = 3, # The number of models to train in parallel\n",
    "                                               hyperparameter_ranges = {\n",
    "                                                    'max_depth': IntegerParameter(6, 10),\n",
    "                                                    'eta'      : ContinuousParameter(0.2, 0.4),\n",
    "                                                    'min_child_weight': IntegerParameter(2, 8),\n",
    "                                                    'subsample': ContinuousParameter(0.7, 1),\n",
    "                                                    'gamma': ContinuousParameter(0, 1),\n",
    "                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea76c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a wrapper around the location of our train and validation data, to make sure that SageMaker\n",
    "# knows our data is in csv format.\n",
    "s3_input_train = TrainingInput(train_location, content_type='csv')\n",
    "s3_input_validation = TrainingInput(val_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55604433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "# Let's train!\n",
    "xgb_hyperparameter_tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dedeb8",
   "metadata": {},
   "source": [
    "Once the hyperamater tuner has finished, we can retrieve information about the best performing model. Since we'd like to set up a batch transform job to test the best model, we can construct a new estimator object from the results of the best training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6efa2e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-10-14 06:38:26 Starting - Preparing the instances for training\n",
      "2021-10-14 06:38:26 Downloading - Downloading input data\n",
      "2021-10-14 06:38:26 Training - Training image download completed. Training in progress.\n",
      "2021-10-14 06:38:26 Uploading - Uploading generated training model\n",
      "2021-10-14 06:38:26 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "xgb_attached = sagemaker.estimator.Estimator.attach(xgb_hyperparameter_tuner.best_training_job())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167aa63",
   "metadata": {},
   "source": [
    "Now that we have our best performing model, we can test it properly. Let's create predictions of our validation set and evaluate them with the roc_auc_score() function.  \n",
    "To start with, we need to build a transformer object from our fit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdf75f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_transformer = xgb_attached.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1233d9db",
   "metadata": {},
   "source": [
    "We need to make sure to provide SageMaker with the type of data that we are providing to our model, in our case text/csv, so that it knows how to serialize our data. In addition, we need to make sure to let SageMaker know how to split our data up into chunks if the entire data set happens to be too large to send to our model all at once.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26721f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of the label part\n",
    "X_val.to_csv(os.path.join(data_dir, 'validation-test.csv'), header=False, index=False)\n",
    "valtest_location = session.upload_data(os.path.join(data_dir, 'validation-test.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_transformer.transform(valtest_location, content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ca524",
   "metadata": {},
   "source": [
    "Now that the batch transform job has finished, the resulting output is stored on S3. Since we wish to analyze the output inside of our notebook we can use a bit of notebook magic to copy the output file from its S3 location and save it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9650778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-766337292225/xgboost-2021-10-14-06-48-37-300/validation-test.csv.out to data_model/validation-test.csv.out\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a920a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.read_csv(os.path.join(data_dir, 'validation-test.csv.out'), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e609dd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUROC -  0.8074147484042518\n"
     ]
    }
   ],
   "source": [
    "print(\"XGBoost AUROC - \", roc_auc_score(Y_val, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a6d51a",
   "metadata": {},
   "source": [
    "Without much suprise, our XGBoost model performed better than the benchmark model:  68% < 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d75a7",
   "metadata": {},
   "source": [
    "### 2) Custom Pytorch ANN \n",
    "\n",
    "Finally, let's build and train a Pytorch ANN. \n",
    "The source code to build, train and predict is stored within the source_pytorch folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33793603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a pytorch estimator\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    source_dir='source_pytorch', \n",
    "                    role=role,\n",
    "                    framework_version=\"1.0\",\n",
    "                    py_version=\"py3\",\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.c4.xlarge',\n",
    "                    output_path='s3://{}/{}'.format(session.default_bucket(), prefix),\n",
    "                    sagemaker_session=session,\n",
    "                    hyperparameters={\n",
    "                        'input_dim': 354,  # num of features\n",
    "                        'hidden_dim1': 512,\n",
    "                        'hidden_dim2': 256,\n",
    "                        'hidden_dim3': 64,\n",
    "                        'output_dim': 1,\n",
    "                        'epochs': 40 # could change to higher\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bba2bae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 19:53:40 Starting - Starting the training job...\n",
      "2021-10-14 19:54:03 Starting - Launching requested ML instancesProfilerReport-1634241220: InProgress\n",
      "...\n",
      "2021-10-14 19:54:40 Starting - Preparing the instances for training............\n",
      "2021-10-14 19:56:41 Downloading - Downloading input data\n",
      "2021-10-14 19:56:41 Training - Downloading the training image...\n",
      "2021-10-14 19:57:04 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-10-14 19:57:00,319 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-10-14 19:57:00,322 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-10-14 19:57:00,335 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-10-14 19:57:01,001 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-10-14 19:57:01,407 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-10-14 19:57:01,407 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-10-14 19:57:01,407 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-10-14 19:57:01,407 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-kfz5jio_/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 21.3 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-10-14 19:57:03,344 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-10-14 19:57:03,357 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"input_dim\": 354,\n",
      "        \"hidden_dim2\": 256,\n",
      "        \"hidden_dim1\": 512,\n",
      "        \"epochs\": 40,\n",
      "        \"hidden_dim3\": 64,\n",
      "        \"output_dim\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2021-10-14-19-53-40-085\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-766337292225/sagemaker-pytorch-2021-10-14-19-53-40-085/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":40,\"hidden_dim1\":512,\"hidden_dim2\":256,\"hidden_dim3\":64,\"input_dim\":354,\"output_dim\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-766337292225/sagemaker-pytorch-2021-10-14-19-53-40-085/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":40,\"hidden_dim1\":512,\"hidden_dim2\":256,\"hidden_dim3\":64,\"input_dim\":354,\"output_dim\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2021-10-14-19-53-40-085\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-766337292225/sagemaker-pytorch-2021-10-14-19-53-40-085/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"40\",\"--hidden_dim1\",\"512\",\"--hidden_dim2\",\"256\",\"--hidden_dim3\",\"64\",\"--input_dim\",\"354\",\"--output_dim\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_INPUT_DIM=354\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM2=256\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM1=512\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=40\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM3=64\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIM=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 40 --hidden_dim1 512 --hidden_dim2 256 --hidden_dim3 64 --input_dim 354 --output_dim 1\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mEpoch: 1, Loss: 0.07881169931962535\u001b[0m\n",
      "\u001b[34mEpoch: 2, Loss: 0.06598975477865666\u001b[0m\n",
      "\u001b[34mEpoch: 3, Loss: 0.06010125172837752\u001b[0m\n",
      "\u001b[34mEpoch: 4, Loss: 0.05481419557194725\u001b[0m\n",
      "\u001b[34mEpoch: 5, Loss: 0.049112116305342485\u001b[0m\n",
      "\u001b[34mEpoch: 6, Loss: 0.040568173242387214\u001b[0m\n",
      "\u001b[34mEpoch: 7, Loss: 0.03187588339971785\u001b[0m\n",
      "\u001b[34mEpoch: 8, Loss: 0.023967454094394266\u001b[0m\n",
      "\u001b[34mEpoch: 9, Loss: 0.01852042127909816\u001b[0m\n",
      "\u001b[34mEpoch: 10, Loss: 0.01345122721160438\u001b[0m\n",
      "\u001b[34mEpoch: 11, Loss: 0.012853501728619099\u001b[0m\n",
      "\u001b[34mEpoch: 12, Loss: 0.011172612467798814\u001b[0m\n",
      "\u001b[34mEpoch: 13, Loss: 0.010198236072612544\u001b[0m\n",
      "\u001b[34mEpoch: 14, Loss: 0.011150508659309489\u001b[0m\n",
      "\u001b[34mEpoch: 15, Loss: 0.007990559200573902\u001b[0m\n",
      "\u001b[34mEpoch: 16, Loss: 0.008079898060653228\u001b[0m\n",
      "\u001b[34mEpoch: 17, Loss: 0.008578769279239076\u001b[0m\n",
      "\u001b[34mEpoch: 18, Loss: 0.009022209525618809\u001b[0m\n",
      "\u001b[34mEpoch: 19, Loss: 0.006173323780587291\u001b[0m\n",
      "\u001b[34mEpoch: 20, Loss: 0.005962570250254781\u001b[0m\n",
      "\u001b[34mEpoch: 21, Loss: 0.004472265948640623\u001b[0m\n",
      "\u001b[34mEpoch: 22, Loss: 0.005672744433502104\u001b[0m\n",
      "\u001b[34mEpoch: 23, Loss: 0.005497688512605325\u001b[0m\n",
      "\u001b[34mEpoch: 24, Loss: 0.004694669495727001\u001b[0m\n",
      "\u001b[34mEpoch: 25, Loss: 0.007049505426731715\u001b[0m\n",
      "\u001b[34mEpoch: 26, Loss: 0.008774779565868147\u001b[0m\n",
      "\u001b[34mEpoch: 27, Loss: 0.004248677883249049\u001b[0m\n",
      "\u001b[34mEpoch: 28, Loss: 0.003265851094886086\u001b[0m\n",
      "\u001b[34mEpoch: 29, Loss: 0.006222723946581142\u001b[0m\n",
      "\u001b[34mEpoch: 30, Loss: 0.004929089546413954\u001b[0m\n",
      "\u001b[34mEpoch: 31, Loss: 0.0066318757376978375\u001b[0m\n",
      "\u001b[34mEpoch: 32, Loss: 0.00491548429965597\u001b[0m\n",
      "\u001b[34mEpoch: 33, Loss: 0.0050746403674138555\u001b[0m\n",
      "\u001b[34mEpoch: 34, Loss: 0.004439628184402903\u001b[0m\n",
      "\u001b[34mEpoch: 35, Loss: 0.00536234411593559\u001b[0m\n",
      "\u001b[34mEpoch: 36, Loss: 0.0031742958207441998\u001b[0m\n",
      "\u001b[34mEpoch: 37, Loss: 0.004432192421166147\u001b[0m\n",
      "\u001b[34mEpoch: 38, Loss: 0.004294971459005135\u001b[0m\n",
      "\u001b[34mEpoch: 39, Loss: 0.004154697699910794\u001b[0m\n",
      "\u001b[34mEpoch: 40, Loss: 0.004417706877377263\u001b[0m\n",
      "\u001b[34m2021-10-14 20:01:01,275 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-10-14 20:02:25 Uploading - Uploading generated training model\n",
      "2021-10-14 20:02:25 Completed - Training job completed\n",
      "ProfilerReport-1634241220: NoIssuesFound\n",
      "Training seconds: 339\n",
      "Billable seconds: 339\n"
     ]
    }
   ],
   "source": [
    "# Train the estimator\n",
    "estimator.fit({'train': s3_input_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eec78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model from the trained estimator data\n",
    "# And point to the prediction script\n",
    "model = PyTorchModel(model_data=estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='1.0',\n",
    "                     py_version=\"py3\",\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='source_pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff68f043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "# Deploy the model to create a predictor\n",
    "PyTorch_predictor = model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30dcc0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "Preds = np.squeeze(np.round(PyTorch_predictor.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db170f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch AUROC -  0.5059479947177575\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch AUROC - \", roc_auc_score(Y_val, Preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff92f2",
   "metadata": {},
   "source": [
    "As expected, whitout taking into account class imbalance the results are really poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a48c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PyTorch_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a3f54",
   "metadata": {},
   "source": [
    "### Kaggle competition\n",
    "Now that we've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle.  \n",
    "Obviously, we will choose the XGBoost model for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4b819ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (18,19) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "mailout_test = pd.read_csv('./data/Udacity_MAILOUT_052018_TEST.csv', sep=';')\n",
    "mailout_test_LNR = mailout_test[\"LNR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78d83c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting columns...\n",
      "Deleting rows...\n",
      "Encoding...\n"
     ]
    }
   ],
   "source": [
    "# Clean the data\n",
    "_, mailout_test = dataset_treatment_test(azdias, mailout_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54981a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "data_dir = \"data_model\"\n",
    "prefix = 'capstone-Arvato'\n",
    "mailout_test.to_csv(os.path.join(data_dir, 'kaggle.csv'), header=False, index=False)\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'kaggle.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbd6ccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-10-14 06:38:26 Starting - Preparing the instances for training\n",
      "2021-10-14 06:38:26 Downloading - Downloading input data\n",
      "2021-10-14 06:38:26 Training - Training image download completed. Training in progress.\n",
      "2021-10-14 06:38:26 Uploading - Uploading generated training model\n",
      "2021-10-14 06:38:26 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "# Recreate a xgb estimator from training job (I manually copy/pasted the best training job from the hyperparameter tuning job)\n",
    "xgb = sagemaker.estimator.Estimator.attach(\"xgboost-211014-0630-006-a000fe15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b064549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a transformer\n",
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578102a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e64fc331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-766337292225/xgboost-2021-10-14-22-39-15-880/kaggle.csv.out to data_model/kaggle.csv.out\n"
     ]
    }
   ],
   "source": [
    "# Download the preds locally\n",
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0ae825c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.read_csv(os.path.join(data_dir, 'kaggle.csv.out'), header=None, names =[\"RESPONSE\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8fc5f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = pd.concat([mailout_test_LNR, pred], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9aee285d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LNR</th>\n",
       "      <th>RESPONSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1754</td>\n",
       "      <td>0.069864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1770</td>\n",
       "      <td>0.037920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1465</td>\n",
       "      <td>0.019774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1470</td>\n",
       "      <td>0.019161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1478</td>\n",
       "      <td>0.020836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    LNR  RESPONSE\n",
       "0  1754  0.069864\n",
       "1  1770  0.037920\n",
       "2  1465  0.019774\n",
       "3  1470  0.019161\n",
       "4  1478  0.020836"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f84c701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result.to_csv(\"data_model/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803743bd",
   "metadata": {},
   "source": [
    "The model scored \"0.74\"  \n",
    "<img src=\"images/Untitled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc40c05",
   "metadata": {},
   "source": [
    "### Possible improvements:\n",
    "1) In terms of data preparation  \n",
    "    - I suspect that some columns had numerical categorical data (like 1 = mean something, 2 = mean something else) which should have been one-encoded  \n",
    "    - There was a lot of missing values within rows. With a more in depth analyse of the meaning of the features, we could have imputed more plausible data (more plausible than just the most common).  \n",
    "    - We could have added (crafted) another feature to the dataset: for each individual, its **cluster number** (from our PCA/K-mean processing). Ofc this feature should also be one-encoded.  \n",
    "     \n",
    "2 ) In terms of modelling  \n",
    "I am quite annoyed for the Pytorch model as I failed to:  \n",
    "    - Implement a hyperparameter tuner (with SageMaker). It could have been interesting to test different combinations of (number of layers, number of neurons, batch-size, lr).  \n",
    "    - Account for the class imbalance. My implementation of the many online examples all returned error messages I haven't been able to solve.  \n",
    "    \n",
    "However, I don't think that an ANN would have had the potential to beat an XGBoost anyway.\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea5997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
